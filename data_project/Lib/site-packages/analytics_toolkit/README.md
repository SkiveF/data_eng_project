# Catalina - Analytics Toolkit - Tools

## Description

This package contains miscellaneous tools that can be useful when creating an Analytics Project.

## Usage

### Configuration

You can access standard variables in this toolkit too. As an example:

````python
from analytics_toolkit.config import DefaultPaths, YellowbrickDatabases

# Accessing databases
databases = YellowbrickDatabases()
db_prod = databases.prod

# Accessing paths to folders (on Windows or Linux)
default_paths = DefaultPaths()
j_drive = default_paths.app_shared
````

### Email

You can send email using your Office 365 account on Catalina.

#### Setup

You will need to add the environment variables "CATALINA_USERNAME" and "CATALINA_PASSWORD". You can
follow [this
tutorial](https://promotastic.atlassian.net/wiki/spaces/DDIM/pages/2969764114/How-to+passwords+for+the+analytics-toolkit+on+VMs)
for more information.

#### Example

Example for sending an email:

````python
import numpy as np
import pandas as pd
from analytics_toolkit.tools import EmailHandler

email_handler = EmailHandler()
email_handler.send(
    to_addrs=["qretour@catmktg.com"],
    subject="Subject of the email,
    text="This is a text email!",
)
````

### Excel

#### Convert an Excel file into PDF (Linux or Windows)

````python
from pathlib import Path

from analytics_toolkit.excel import convert_excel_to_pdf

# Convert an Excel file into PDF
pdf_filepath = convert_excel_to_pdf(
    Path("your/file/path.xlsx"), 
    destination=Path("new/pdf/file.pdf"),
    keep_sheets=[1, 2, 3],
)
````

#### Remove the last line jump from a .csv file

This is especially useful when creating targeting files that you need to export.

````python
from pathlib import Path

from analytics_toolkit.excel import remove_last_line_from_csv

# Remove last line from CSV file
csv_file = Path("your/csv/file.csv")
remove_last_line_from_csv(csv_file)
````

### Execution

#### Getting the analytics logger

This is especially useful to log the messages with different levels (INFO, WARNING, ERROR ...) and
to easily keep track of the time, to log into a separate file, etc.

````python
from analytics_toolkit.execution import get_analytics_logger

logger = get_analytics_logger()
logger.info("Hello World!")
logger.error("Error!!")
````

#### Monitoring the execution of a function

This is especially useful to monitor when, with what parameters and how long did a function take to execute.

````python
from analytics_toolkit.execution import get_analytics_logger

@log_execution(log_execution_time=True, log_signature=True)
def sum(a, b):
    return a + b

sum(1, 2)
````

### SAS

This toolkit contains some functions to work with SAS files

#### Reading .sas7bdat files

You can easily read .sas data files:

````python
from pathlib import Path

from analytics_toolkit.sas import read_sas

filepath = Path("/to/your/file.sas7bdat")

# Two possibilities - if you need metadata or not.
df, metadata = read_sas(filepath)
df = read_sas(filepath)[0]
````

#### Opening a .egp project without SAS Enterprise Guide

You can also open a SAS .egp file and explode it into the .sas sub-files by using the code below.
This will create a new folder at the same place as the project, with the .sas sub-programs that are
in the .egp project.

````python
from pathlib import Path

from analytics_toolkit.sas import explode_sas_project

filepath = Path("to/your/project.egp")
explode_sas_project(filepath)
````

### SQL

#### Checking if a query is a SQL SELECT statement or table name

````python
from analytics_toolkit.sql import is_sql_select_query, is_table_name

assert is_sql_select_query("SELECT * from table_name") is True
assert is_sql_select_query("Not a SQL SELECT statement") is False
assert is_table_name("a_table_name") is True
assert is_table_name("Not a TABLE NAME!") is False
````

#### Counting the number of affected rows in a SELECT / UPDATE / DELETE SQL statement

````python
from analytics_toolkit.database.connectors import YellowbrickDatabaseConnector
from analytics_toolkit.sql import get_n_affected_rows

connector = YellowbrickDatabaseConnector(database="py3frta1")
table_name, query = "test_table_2", "SELECT * FROM test_table_1 LIMIT 100"
create_result = connector.execute_query(f"CREATE TABLE {table_name} AS ({query})")

assert get_n_affected_rows(create_result) == 100
````

#### Reading / formatting .sql files

If you want to read & parameterize .sql files, you can use the code below. The reader will:

- Automatically format database names using the config.py's YellowbrickDatabases: {db_prod}.table -> py3frpa1.public.table
- If you pass a DataClass to the tables argument, containing attributes with name x and value y for example, {table_x} -> y
- If you pass a DataClass to the dates argument, containing attributes with name x and value y for example, {date_x} -> y

````python
from pathlib import Path

from analytics_toolkit.sql import SQLReader

sql_file = Path("to/your/file.sql")
sql_reader = SQLReader()

# If your file contains only one SQL statement
sql_statement = sql_reader.read_query(sql_file)

# If your file contains multiple SQL statements
sql_statements = sql_reader.read_queries(sql_file, keys=["your", "sub", "queries"])
````

## TO-DO list

- Logging: add the file name on which to log the logs
