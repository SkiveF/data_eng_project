# Catalina - Yellowbrick connector

## Description

This connector allows to read and write tables on Yellowbrick efficiently and seamlessly.

## Authentication

To use the connector, you can either :

1. [OPTIMAL] If your are using your Catalina credentials for Yellowbrick, you can fill the CATALINA_USERNAME / CATALINA_PASSWORD environment variables. They will also be used for other purposes (such as sending emails for instance)
2. [GOOD] Add the YELLOWBRICK_USERNAME / YELLOWBRICK_PASSWORD environment variables.
3. [NOT ADVISED] Input your username / password as parameters in the creation of the connector.

To add environment variables, follow this [link](https://promotastic.atlassian.net/wiki/spaces/DDIM/pages/2969764114/How-to+passwords+for+the+analytics-toolkit+on+VMs).

## Usage examples

````python
import numpy as np
import pandas as pd

from analytics_toolkit.database.connectors import YellowbrickDatabaseConnector

databases = {
    'PA1': f"py3frpa1",  # production
    'SA1': f"py3frsa1",  # permanent sandbox - no table deletion
    'TA1': f"py3frta1",  # temporary sandbox - tables deleted often
}
database = databases["TA1"]  # choose the table you want to write on
yellowbrick = YellowbrickDatabaseConnector(database=database)

# Inputs that could be used
table_name = "your_table_name"
new_table_name = "new_table_name"
sql_query = f"SELECT * FROM {table_name} LIMIT 100"

# Get a table from a table name
df_table = yellowbrick.get_table(table_name)

# Get a table from a SQL Query defining this table
df_query = yellowbrick.get_table(sql_query)

# Create a new table based on a SQL query
yellowbrick.create_table(sql_query, new_table_name, if_exists=f"replace")

# Create a new table based on a DataFrame
df = pd.DataFrame(np.ones(shape=(100, 5)), columns=list("abcde"))
yellowbrick.create_table(df, new_table_name)

# Drop a table
yellowbrick.drop_table(table_name)

# Check if a table exists
boolean = yellowbrick.has_table(table_name)

# Get columns of a table
column_list = list(yellowbrick.get_columns(table_name))

# Get data types of a table
column_dtypes = yellowbrick.get_columns(table_name)

# Drop columns of a table directly in Yellowbrick
drop_columns = []
yellowbrick.drop_columns(table_name, drop_columns)
````

## Advanced usage of SQLAlchemy engine

This wrapper is based on SQLAlchemy. You can use many functionalities described in [this
  tutorial](https://docs.sqlalchemy.org/en/14/orm/tutorial.html) or [this
  one](https://towardsdatascience.com/sqlalchemy-python-tutorial-79a577141a91).

````python
from analytics_toolkit.database.connectors import YellowbrickDatabaseConnector

# You can directly use the SQLAlchemy engine
yellowbrick = YellowbrickDatabaseConnector(database=f"your_database")
sqlalchemy_engine = yellowbrick.engine
````

## TO-DOs

- Leave pd.read_sql as such -> repair the sqlalchemy metadata returning nothing
- Add schema handling with pd.to_sql and ybload / unload
- Use DTypes instead of testing a small query -> repair the sqlalchemy metadata returning nothing
- Extract metadata information :
  - SELECT * FROM information_schema.columns WHERE TABLE_NAME = '{table_name}'

First try with SQLAlchemy:

````python
with self.get_connection() as connection:
    db = SQLDatabase(connection)
    table = db.get_table(table_name=table_name)  # type: SQLTable
    dtypes = {col.name: col.type for col in table.columns}
````

YBUnload: commands [here](https://www.yellowbrick.com/docs/4.1/unloading/ybunload_options.html)

- Better dtype handling using methods from Pandas
- Use **compress** argument to lower size to pass to the network
- Use **parallel** argument to speed up the unloading
- Add a proper logger of YBTools

YBLoad: commands [here](https://www.yellowbrick.com/docs/4.1/bulk_loading/ybload_options.html)

- Change compression method to GZIP ? No need to specify anything else, apart from maybe **source-compression**

Speed up writing on base:

- Modify the insertion method to use
  a [callable](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#insertion-method)
  (COPY FROM statement?)
- Inspire from the load_data method on snow_utils

Speed up reading on base:

- Modify pd.read_sql to pass by cursor.copy_expert() (tutorial [here](https://towardsdatascience.com/optimizing-pandas-read-sql-for-postgres-f31cd7f707ab))
