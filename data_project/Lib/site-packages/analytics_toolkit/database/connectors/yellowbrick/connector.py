""" Methods specific to the Yellowbrick database connector. """
import os
from concurrent.futures import ProcessPoolExecutor
from datetime import datetime
from functools import partial
from pathlib import Path
from typing import Optional, Union

import numpy as np
import pandas as pd
from analytics_toolkit.sql import is_table_name
from sqlalchemy import create_engine
from sqlalchemy.dialects import registry
from sqlalchemy.engine import URL
from sqlalchemy.engine.base import Engine

from ..generic import DatabaseConnector, SQLAlchemyType
from .dialect import YellowbrickDialect
from .ybtools import YellowbrickTools


class YellowbrickDatabaseConnector(DatabaseConnector):
    """
    Connector to use Yellowbrick in Python
    """

    def __init__(
        self,
        database: str,
        username: str = None,
        password: str = None,
        host: str = None,
        port: int = None,
        use_ybtools: bool = True,
    ):
        """
        Instantiates a YellowbrickDatabaseConnector.

        Args:
            database (str): db to use for write, as you can change it if you need to read other dbs.
            username (str, optional): Yellowbrick username (normally @catmktg.com username).
                If None, tries the env variable "YELLOWBRICK_USERNAME" or prompts the user.
            password (str, optional): Yellowbrick password (given by support).
                If None, tries the env variable "YELLOWBRICK_PASSWORD" or prompts the user.
            host (str, optional): host of the Yellowbrick server. Defaults to "prodybc"
            port (int, optional): port of the Yellowbrick server. Defaults to 5432.
            use_ybtools (bool, optional): Defaults to True. If False, does not try to use ybtools.
        """
        super().__init__(app_name="Yellowbrick", username=username, password=password)

        # In Catalina, as of Sept. 2021, all DBs have lowercase names, so lowering it all the time.
        self.database = database.lower()
        self.host = host if host is not None else "prodybc"
        self.port = port if port is not None else 5432

        # SQLAlchemy engine
        self.engine = self.get_sqlalchemy_engine()

        # ybtools parameters
        self.use_ybtools = use_ybtools
        if self.use_ybtools:
            self.ybtools = YellowbrickTools(
                username=self.username,
                password=self.password,
                database=self.database,
                host=self.host,
            )

        # Multiprocessing parameters
        self.n_processors = os.cpu_count()

    def get_sqlalchemy_engine(self) -> Engine:
        """Sets up the SQLAlchemy engine for the Yellowbrick database connector"""
        registry.register(
            self.app_name.lower(),
            YellowbrickDialect.__module__,
            YellowbrickDialect.__name__,
        )
        database_url = URL.create(
            drivername=self.app_name.lower(),
            username=self.username,
            password=self.password,
            host=self.host,
            port=self.port,
            database=self.database,
        )
        return create_engine(database_url)

    def get_table(self, sql: str) -> pd.DataFrame:
        if self.use_ybtools and "ybunload" in self.ybtools.active_programs:
            return self.get_table_using_ybunload(sql)
        else:
            return super().get_table(sql)

    def get_table_using_ybunload(
        self, sql: str, ybunload_options: dict[str, Optional[str]] = None
    ) -> pd.DataFrame:
        """
        Gets a table using ybunload.

        When getting a table by name, before ybunload, calls the request with limit 1 to get the
        column names and the data types associated. It is not the best way to do it as it might be
        slow (1 SQL query + 1 ybunload). TODO: find another way to get column names and data types?

        When getting a table from a SQL query, there are three steps: 1) Creates a table from query
        2) Gets table from table name using ybunload 3) Drop the temporary table.

        This SQL query method is interesting as in YBUnload for now, we need to do a first SQL query
        with limit 5000 to get data types of the table. For big SQL queries, it might lead to
        executing the query twice and is very long for complex queries with JOIN statements, etc.

        However, this means the method getting a table from a SQL Query won't return ordered tables,
        even if asked so in a ORDER BY statement. We are sacrificing ordering, in order to avoid
        running the query twice (for data type gathering).

        Args:
            sql (str): SQL query or SQL table name. Both would work!
            ybunload_options (dict[str, str], optional): Additional parameters to pass to
                ybunload.exe. Defaults to None.

        Returns:
            pd.DataFrame: DataFrame of the given table or SQL query.
        """
        if is_table_name(sql):
            return self._get_table_from_name_using_ybunload(
                sql, ybunload_options=ybunload_options
            )
        else:
            return self._get_table_from_query_using_ybunload(
                sql, ybunload_options=ybunload_options
            )

    def _get_table_from_name_using_ybunload(
        self, table_name: str, ybunload_options: dict[str, Optional[str]] = None
    ) -> pd.DataFrame:
        """Private method for getting table by name using ybunload. See docstring above."""
        # TODO: final integration for bool columns with NAs -> need Metadata information
        # TODO: use information_schema instead of query with limit 5 000
        # TODO: careful! two different data types are returned if DF <= or > 5000 lines.
        # Better in some cases (if full NULL -> str) and worse in some (int -> float).

        # ---- Extract information DataFrame for column names and data types ---- #
        # We limit 5 000 in order to get the real types of the data. If we limit 1, we risk to get
        # only NAs and then, not know what the dtype is. 1 or 5 000 is similar in terms of time.
        # It also accelerates computation: if we have less than 5000 lines, no need for ybtools.
        df_info = super().get_table(f"SELECT * FROM {table_name} LIMIT 5000")
        if len(df_info) < 5000:
            return df_info

        # We isolate date type columns to use them in read_csv.
        # We isolate int column to force their type to nullable int.
        datetime_dtypes = ["datetime", "timedelta", "datetimetz"]
        date_cols = df_info.select_dtypes(include=datetime_dtypes).columns.tolist()
        int_dtypes = {col: "Int64" for col in df_info.select_dtypes(include=["int"])}
        other_dtypes = {
            col: dtype
            for col, dtype in df_info.dtypes.items()
            if col not in date_cols + list(int_dtypes.keys())
        }

        # Extract data using ybunload to specific files
        ybunload_options = ybunload_options if ybunload_options is not None else {}
        self.ybtools.ybunload(table_name, **ybunload_options)

        # Find all files related to the DataFrame - if None, returns df_info to get proper columns
        files_to_check = [
            self.ybtools.output_dir / filename
            for filename in os.listdir(self.ybtools.output_dir)
            if filename.startswith(self.ybtools.prefix)
        ]
        files_to_check = sorted(files_to_check, key=self._extract_ybunload_file_number)
        if not files_to_check:
            return df_info

        # Read all CSVs using multiprocessing.
        # TODO: remove multiprocessing if only one file?
        # TODO: add a threshold (ybtools file size) for when to use multiprocessing?
        with ProcessPoolExecutor() as executor:
            func = partial(
                pd.read_csv,
                index_col=None,
                header=None,
                names=df_info.columns,
                dtype=other_dtypes | int_dtypes,
                parse_dates=date_cols,
            )
            dfs_output = executor.map(func, files_to_check)
            df_output = pd.concat(dfs_output, ignore_index=True)

        # Delete files at the end
        for file in files_to_check:
            os.remove(file)

        return df_output

    def _extract_ybunload_file_number(self, file: Path) -> tuple[int, int]:
        """Extracts the file number generated from a ybunload process."""
        numbers = file.stem.removeprefix(self.ybtools.prefix).strip("_").split("_")
        return int(numbers[0]), int(numbers[1])

    def _get_table_from_query_using_ybunload(
        self, query: str, ybunload_options: dict[str, Optional[str]] = None
    ) -> pd.DataFrame:
        """Private method for getting table by query using ybunload. See docstring above."""
        with self._temp_table_name() as temp_table_name:
            self.create_table(query, temp_table_name)
            df_table = self._get_table_from_name_using_ybunload(
                temp_table_name, ybunload_options=ybunload_options
            )

        return df_table

    def execute_query(self, query: str):
        """Executes a SQL query. Overrides the parent method."""
        if self.use_ybtools and "ybsql" in self.ybtools.active_programs:
            result = self.ybtools.ybsql(query)
        else:
            result = super().execute_query(query)

        return result

    def create_table(
        self,
        data_or_query: Union[str, pd.DataFrame, pd.Series],
        table_name: str,
        if_exists: str = "fail",
        dtype: dict[str, Union[str, SQLAlchemyType]] = None,
        grant_to: Optional[str] = "all",
    ):
        # Defaulting grant_to to "all" in Yellowbrick
        return super().create_table(
            data_or_query,
            table_name,
            if_exists=if_exists,
            dtype=dtype,
            grant_to=grant_to,
        )

    def create_table_from_df(
        self,
        df: Union[pd.DataFrame, pd.Series],
        table_name: str,
        if_exists: str = "fail",
        dtype: dict[str, Union[str, SQLAlchemyType]] = None,
        grant_to: Optional[str] = "all",
    ):
        if self.use_ybtools and "ybload" in self.ybtools.active_programs:
            self.create_table_from_df_using_ybload(
                df, table_name, if_exists=if_exists, dtype=dtype, grant_to=grant_to
            )
        else:
            super().create_table_from_df(
                df, table_name, if_exists=if_exists, dtype=dtype, grant_to=grant_to
            )

    def create_table_from_df_using_ybload(
        self,
        df: pd.DataFrame,
        table_name: str,
        if_exists: str = "fail",
        dtype: dict[str, Union[str, SQLAlchemyType]] = None,
        grant_to: Optional[str] = "all",
        ybload_options: dict[str, Optional[str]] = None,
    ):
        """
        Uses ybload to accelerate the writing process for large tables. Three possible options:
        - Table memory usage below 300KB: use SQLAlchemy directly instead of YBLoad
        - Table memory usage between 300KB and 10MB: use YBTools, without parallelization
        - Table memory usage above 10MB: use YBTools, with parallelization

        For the two last cases, first creates an empty table with appropriate
        column names, then loads the data in the DataFrame into this table.

        Args:
            df (pd.DataFrame): DataFrame to write into the table
            table_name (str): name of the table to write
            if_exists (str, optional): "fail" will raise an error, "replace" will replace the table,
                "append" will append the data to the table. Defaults to "fail".
            ybload_options (Optional[str]): Additional options to pass to ybload. You will probably
                need to pass them using **. Example: **{"--logfile": "ybtools/log/load.txt"}
                More options: https://www.yellowbrick.com/docs/5.2/bulk_loading/ybload_options.html
            grant_to (str, optional): entities to give access to. Defaults to None.
                In 0.6.0, only "all" or None is supported.
        """
        # Some minor formatting at the beginning
        table_name = self.format_table_name(table_name)
        df = self.format_data(df)

        # If low memory usage, just use SQLAlchemy directly.
        if (memory_usage := df.memory_usage(deep=True).sum()) < 300 * 10**3:
            return super().create_table_from_df(
                df, table_name, if_exists=if_exists, dtype=dtype, grant_to=grant_to
            )

        # If not, use YBTools.
        # First, creates the empty table if needed.
        super().create_table_from_df(
            df[:0], table_name, if_exists=if_exists, dtype=dtype, grant_to=grant_to
        )

        # Then, writes the data into CSVs.
        if memory_usage < 10 * 10**6:
            filepaths = self._write_to_csv_single_processing(df)
        else:
            filepaths = self._write_to_csv_multiprocessing(df)

        # Last, tries to run ybload. Even if it failed, still removes the .csv files
        try:
            ybload_options = ybload_options if ybload_options is not None else {}
            self.ybtools.ybload(table_name, *filepaths, **ybload_options)
        finally:
            for filepath in filepaths:
                os.remove(filepath)

    def _write_to_csv_single_processing(self, df: pd.DataFrame) -> list[Path]:
        """Writes the YBLoad .csv using one processor."""
        suffix = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = self.ybtools.output_dir / f"YBLOAD_{suffix}.csv"
        df.to_csv(filepath)

        return [filepath]

    def _write_to_csv_multiprocessing(self, df: pd.DataFrame) -> list[Path]:
        """Writes the YBLoad .csvs using multiple processors."""
        suffix = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepaths = [
            self.ybtools.output_dir / f"YBLOAD_{suffix}_{i}.csv"
            for i in range(self.n_processors)
        ]
        dfs_and_filepaths = zip(np.array_split(df, self.n_processors), filepaths)
        with ProcessPoolExecutor() as executor:  # list is for the generator to execute
            list(executor.map(self._df_to_csv, dfs_and_filepaths))

        return filepaths

    @staticmethod
    def _df_to_csv(df_and_path: tuple[pd.DataFrame, Path]):
        df, path = df_and_path
        return df.to_csv(path)

    def _drop_columns(
        self, table_name: str, columns: list[str], table_columns: list[str]
    ):
        """Private method to drop columns in a table. Can be overridden."""
        keep_columns = [col for col in table_columns if col not in columns]
        query = f"SELECT {', '.join(keep_columns)} FROM {table_name}"
        self.create_table(query, table_name, if_exists="replace")
